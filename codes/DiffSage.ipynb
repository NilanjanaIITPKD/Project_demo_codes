{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFd-VQbiiJZ2",
    "outputId": "2db63f6d-b0ea-435d-a74b-f43ad2feec65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Collecting torch-scatter\n",
      "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=3577503 sha256=bba965877ee29edd39f51c75e05b3179cb2b1768fa8c045b8541021dbca2a0d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.0.9\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Collecting torch-sparse\n",
      "  Downloading torch_sparse-0.6.13.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 5.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
      "Building wheels for collected packages: torch-sparse\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-sparse: filename=torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl size=1710390 sha256=9945ffdf1a925f07e66a7d15d5acb10b5ddbc7a244f0eb729549fad830c1e25e\n",
      "  Stored in directory: /root/.cache/pip/wheels/e0/01/be/6b2966e0ff20bb023ae35e5d17903e6e5b4df46dd5892f6be6\n",
      "Successfully built torch-sparse\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.6.13\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
      "\u001b[K     |████████████████████████████████| 407 kB 30.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "Building wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=7bbdd0166ae587cbbb44ea3c74a2bfd09982eee93a3db9d249deba0c9bfd0ac1\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZkh0RjDnbJU",
    "outputId": "0233c1c3-f5eb-46e2-c55d-336b715bb670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: TrainAcc: 0.705945 TrainLoss:0.595837 ValAcc:0.692308 BestTestAcc: 0.764706\n",
      "Epoch1: TrainAcc: 0.751592 TrainLoss:0.592268 ValAcc:0.709402 BestTestAcc: 0.781513\n",
      "Epoch5: TrainAcc: 0.771762 TrainLoss:0.588028 ValAcc:0.717949 BestTestAcc: 0.747899\n",
      "Epoch6: TrainAcc: 0.780255 TrainLoss:0.591335 ValAcc:0.726496 BestTestAcc: 0.773109\n",
      "Epoch10: TrainAcc: 0.788747 TrainLoss:0.589115 ValAcc:0.743590 BestTestAcc: 0.731092\n",
      "Epoch0: TrainAcc: 0.681529 TrainLoss:0.598839 ValAcc:0.632479 BestTestAcc: 0.722689\n",
      "Epoch1: TrainAcc: 0.757962 TrainLoss:0.594708 ValAcc:0.692308 BestTestAcc: 0.806723\n",
      "Epoch4: TrainAcc: 0.750531 TrainLoss:0.595495 ValAcc:0.709402 BestTestAcc: 0.781513\n",
      "Epoch7: TrainAcc: 0.784501 TrainLoss:0.591681 ValAcc:0.726496 BestTestAcc: 0.798319\n",
      "Epoch11: TrainAcc: 0.797240 TrainLoss:0.588432 ValAcc:0.760684 BestTestAcc: 0.773109\n",
      "Epoch12: TrainAcc: 0.799363 TrainLoss:0.587725 ValAcc:0.777778 BestTestAcc: 0.798319\n",
      "Epoch0: TrainAcc: 0.722930 TrainLoss:0.595904 ValAcc:0.683761 BestTestAcc: 0.714286\n",
      "Epoch2: TrainAcc: 0.763270 TrainLoss:0.592417 ValAcc:0.717949 BestTestAcc: 0.773109\n",
      "Epoch3: TrainAcc: 0.771762 TrainLoss:0.592454 ValAcc:0.743590 BestTestAcc: 0.789916\n",
      "Epoch4: TrainAcc: 0.770701 TrainLoss:0.594958 ValAcc:0.752137 BestTestAcc: 0.789916\n",
      "Epoch6: TrainAcc: 0.776008 TrainLoss:0.592149 ValAcc:0.760684 BestTestAcc: 0.756303\n",
      "Epoch7: TrainAcc: 0.789809 TrainLoss:0.594398 ValAcc:0.794872 BestTestAcc: 0.739496\n",
      "Epoch0: TrainAcc: 0.712314 TrainLoss:0.598927 ValAcc:0.683761 BestTestAcc: 0.705882\n",
      "Epoch1: TrainAcc: 0.761146 TrainLoss:0.595413 ValAcc:0.692308 BestTestAcc: 0.739496\n",
      "Epoch2: TrainAcc: 0.767516 TrainLoss:0.590935 ValAcc:0.709402 BestTestAcc: 0.747899\n",
      "Epoch8: TrainAcc: 0.777070 TrainLoss:0.586273 ValAcc:0.726496 BestTestAcc: 0.789916\n",
      "Epoch16: TrainAcc: 0.809979 TrainLoss:0.589796 ValAcc:0.786325 BestTestAcc: 0.789916\n",
      "Epoch0: TrainAcc: 0.710191 TrainLoss:0.599240 ValAcc:0.683761 BestTestAcc: 0.764706\n",
      "Epoch2: TrainAcc: 0.742038 TrainLoss:0.593618 ValAcc:0.700855 BestTestAcc: 0.747899\n",
      "Epoch4: TrainAcc: 0.770701 TrainLoss:0.591115 ValAcc:0.709402 BestTestAcc: 0.773109\n",
      "Epoch5: TrainAcc: 0.760085 TrainLoss:0.588117 ValAcc:0.717949 BestTestAcc: 0.747899\n",
      "Epoch10: TrainAcc: 0.780255 TrainLoss:0.585587 ValAcc:0.735043 BestTestAcc: 0.781513\n",
      "Epoch13: TrainAcc: 0.785563 TrainLoss:0.594098 ValAcc:0.743590 BestTestAcc: 0.722689\n",
      "Epoch22: TrainAcc: 0.782378 TrainLoss:0.598204 ValAcc:0.752137 BestTestAcc: 0.773109\n",
      "Epoch26: TrainAcc: 0.783439 TrainLoss:0.594512 ValAcc:0.760684 BestTestAcc: 0.781513\n",
      "Epoch0: TrainAcc: 0.663482 TrainLoss:0.597035 ValAcc:0.683761 BestTestAcc: 0.722689\n",
      "Epoch3: TrainAcc: 0.733546 TrainLoss:0.596970 ValAcc:0.692308 BestTestAcc: 0.731092\n",
      "Epoch4: TrainAcc: 0.769639 TrainLoss:0.597709 ValAcc:0.709402 BestTestAcc: 0.789916\n",
      "Epoch7: TrainAcc: 0.750531 TrainLoss:0.594294 ValAcc:0.735043 BestTestAcc: 0.756303\n",
      "{0.001: 0.7310924369747899, 0.0001: 0.7983193277310925, 1e-05: 0.7394957983193278, 1e-06: 0.7899159663865546, 0.01: 0.7815126050420168, 0.1: 0.7563025210084033}\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.nn import dense_diff_pool, DenseSAGEConv\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_layer, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layer):\n",
    "            if i == 0:\n",
    "                self.layers.append(DenseSAGEConv(in_dim, hidden_dim))\n",
    "            elif i == num_layer - 1:\n",
    "                self.layers.append(DenseSAGEConv(hidden_dim, out_dim))\n",
    "            else:\n",
    "                self.layers.append(DenseSAGEConv(hidden_dim, hidden_dim))\n",
    "\n",
    "            if i != num_layer - 1:\n",
    "                self.layers.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            else:\n",
    "                self.layers.append(torch.nn.BatchNorm1d(out_dim))\n",
    "\n",
    "    def forward(self, x, a, mask=None):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseSAGEConv):\n",
    "                x = layer(x, a, mask)\n",
    "            else:\n",
    "                batch_size, num_nodes, num_channels = x.size()\n",
    "                x = x.view(-1, num_channels)\n",
    "                x = layer(x)\n",
    "                x = x.view(batch_size, num_nodes, num_channels)\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_cluster):\n",
    "        super().__init__()\n",
    "        self.embedding = GNN(3, hidden_dim, hidden_dim, hidden_dim)\n",
    "        self.assignment = GNN(3, hidden_dim, hidden_dim, num_cluster)\n",
    "\n",
    "    def forward(self, x, a, mask=None):\n",
    "        z = self.embedding(x, a, mask)\n",
    "        s = self.assignment(x, a, mask)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        return dense_diff_pool(z, a, s, mask)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset, max_node, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.max_node = max_node\n",
    "        pool1_cluster = ceil(.25 * self.max_node)\n",
    "        pool2_cluster = ceil(.25 * pool1_cluster)\n",
    "        self.gnn_before_p1 = GNN(2, dataset.num_features, hidden_dim, hidden_dim)\n",
    "        self.gnn_after_p1 = GNN(3, hidden_dim, hidden_dim, hidden_dim)\n",
    "        self.gnn_after_p2 = GNN(3, hidden_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "        self.p1 = DiffPool(hidden_dim, pool1_cluster)\n",
    "        self.p2 = DiffPool(hidden_dim, pool2_cluster)\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                                       torch.nn.BatchNorm1d(hidden_dim // 2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_dim // 2, dataset.num_classes))\n",
    "\n",
    "    def forward(self, x, a, mask=None):\n",
    "        x = self.gnn_before_p1(x, a, mask)\n",
    "        x, a, l1, e1 = self.p1(x, a, mask)\n",
    "        x = self.gnn_after_p1(x, a)\n",
    "        x, a, l2, e2 = self.p2(x, a)\n",
    "        x = self.gnn_after_p2(x, a)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.mlp(x), l1 + l2 + e1 + e2\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        x, mask = to_dense_batch(data.x, data.batch)\n",
    "        adj = to_dense_adj(data.edge_index, data.batch)\n",
    "        pred = model(x, adj, mask)[0].max(dim=1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "experiment_name = 'DiffPool'\n",
    "input_path = '/kaggle/working/data'\n",
    "output_path = '/kaggle/working/'\n",
    "results = dict()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "dataset = TUDataset(input_path, 'DD')\n",
    "train_cnt = int(.8 * len(dataset))\n",
    "val_cnt = int(.1 * len(dataset))\n",
    "test_cnt = len(dataset) - train_cnt - val_cnt\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_cnt, val_cnt, test_cnt],\n",
    "                                                                torch.random.manual_seed(0))\n",
    "max_node = 0\n",
    "for data in dataset:\n",
    "    if data.num_nodes > max_node:\n",
    "        max_node = data.num_nodes\n",
    "\n",
    "for wd in [1e-3,1e-4,1e-5,1e-6,1e-2,1e-1]:        \n",
    "    hidden_dim=64\n",
    "    model = Net(dataset, max_node, hidden_dim).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay=wd)\n",
    "    train_set = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "    val_set = DataLoader(val_data)\n",
    "    test_set = DataLoader(test_data)\n",
    "    epoch = 30\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    epoch_to_break = 0\n",
    "\n",
    "    for i in range(epoch):\n",
    "        train_sum_acc = 0\n",
    "        train_cnt = 0\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        for data in train_set:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x, mask = to_dense_batch(data.x, data.batch)\n",
    "            adj = to_dense_adj(data.edge_index, data.batch)\n",
    "            result, lpe_loss = model(x, adj, mask)\n",
    "            loss = criterion(result, data.y) + lpe_loss\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = result.max(dim=1)[1]\n",
    "            acc = torch.eq(pred, data.y).sum().item()\n",
    "            train_sum_acc += acc\n",
    "            train_cnt += len(data.y)\n",
    "\n",
    "        val_acc = test(model, val_set, device)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test(model, test_set, device)\n",
    "            epoch_to_break = 0\n",
    "            print(\n",
    "            f'Epoch{i}: TrainAcc: {train_sum_acc / train_cnt:.6f} TrainLoss:{train_loss / train_cnt:.6f} ValAcc:{val_acc:.6f}'\n",
    "            f' BestTestAcc: {best_test_acc:.6f}')\n",
    "        else:\n",
    "            epoch_to_break += 1\n",
    "\n",
    "        if epoch_to_break >= 10:\n",
    "            break\n",
    "    results[wd]=best_test_acc\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRGP4g9Xb8uD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DiffSage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
