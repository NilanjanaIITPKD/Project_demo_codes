{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFd-VQbiiJZ2",
        "outputId": "2db63f6d-b0ea-435d-a74b-f43ad2feec65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=3577503 sha256=bba965877ee29edd39f51c75e05b3179cb2b1768fa8c045b8541021dbca2a0d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.13.tar.gz (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl size=1710390 sha256=9945ffdf1a925f07e66a7d15d5acb10b5ddbc7a244f0eb729549fad830c1e25e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/01/be/6b2966e0ff20bb023ae35e5d17903e6e5b4df46dd5892f6be6\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=7bbdd0166ae587cbbb44ea3c74a2bfd09982eee93a3db9d249deba0c9bfd0ac1\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code achieves a performance of around 96.60%. However, it is not\n",
        "# directly comparable to the results reported by the TGN paper since a\n",
        "# slightly different evaluation setup is used here.\n",
        "# In particular, predictions in the same batch are made in parallel, i.e.\n",
        "# predictions for interactions later in the batch have no access to any\n",
        "# information whatsoever about previous interactions in the same batch.\n",
        "# On the contrary, when sampling node neighborhoods for interactions later in\n",
        "# the batch, the TGN paper code has access to previous interactions in the\n",
        "# batch.\n",
        "# While both approaches are correct, together with the authors of the paper we\n",
        "# decided to present this version here as it is more realsitic and a better\n",
        "# test bed for future methods.\n",
        "\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from torch.nn import Linear\n",
        "\n",
        "from torch_geometric.datasets import JODIEDataset\n",
        "from torch_geometric.loader import TemporalDataLoader\n",
        "from torch_geometric.nn import TGNMemory, TransformerConv\n",
        "from torch_geometric.nn.models.tgn import (\n",
        "    IdentityMessage,\n",
        "    LastAggregator,\n",
        "    LastNeighborLoader,\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'JODIE')\n",
        "dataset = JODIEDataset(\"../\", name='wikipedia')\n",
        "data = dataset[0]\n",
        "\n",
        "# For small datasets, we can put the whole dataset on GPU and thus avoid\n",
        "# expensive memory transfer costs for mini-batches:\n",
        "data = data.to(device)\n",
        "\n",
        "# Ensure to only sample actual destination nodes as negatives.\n",
        "min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
        "train_data, val_data, test_data = data.train_val_test_split(\n",
        "    val_ratio=0.15, test_ratio=0.15)\n",
        "\n",
        "train_loader = TemporalDataLoader(train_data, batch_size=200)\n",
        "val_loader = TemporalDataLoader(val_data, batch_size=200)\n",
        "test_loader = TemporalDataLoader(test_data, batch_size=200)\n",
        "\n",
        "neighbor_loader = LastNeighborLoader(data.num_nodes, size=10, device=device)\n",
        "\n",
        "\n",
        "class GraphAttentionEmbedding(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
        "        super().__init__()\n",
        "        self.time_enc = time_enc\n",
        "        edge_dim = msg_dim + time_enc.out_channels\n",
        "        self.conv = TransformerConv(in_channels, out_channels // 2, heads=2,\n",
        "                                    dropout=0.1, edge_dim=edge_dim)\n",
        "\n",
        "    def forward(self, x, last_update, edge_index, t, msg):\n",
        "        rel_t = last_update[edge_index[0]] - t\n",
        "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
        "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
        "        return self.conv(x, edge_index, edge_attr)\n",
        "\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.lin_src = Linear(in_channels, in_channels)\n",
        "        self.lin_dst = Linear(in_channels, in_channels)\n",
        "        self.lin_final = Linear(in_channels, 1)\n",
        "\n",
        "    def forward(self, z_src, z_dst):\n",
        "        h = self.lin_src(z_src) + self.lin_dst(z_dst)\n",
        "        h = h.relu()\n",
        "        return self.lin_final(h)\n",
        "\n",
        "\n",
        "memory_dim = time_dim = embedding_dim = 100\n",
        "\n",
        "memory = TGNMemory(\n",
        "    data.num_nodes,\n",
        "    data.msg.size(-1),\n",
        "    memory_dim,\n",
        "    time_dim,\n",
        "    message_module=IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
        "    aggregator_module=LastAggregator(),\n",
        ").to(device)\n",
        "\n",
        "gnn = GraphAttentionEmbedding(\n",
        "    in_channels=memory_dim,\n",
        "    out_channels=embedding_dim,\n",
        "    msg_dim=data.msg.size(-1),\n",
        "    time_enc=memory.time_enc,\n",
        ").to(device)\n",
        "\n",
        "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    set(memory.parameters()) | set(gnn.parameters())\n",
        "    | set(link_pred.parameters()), lr=0.0001)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Helper vector to map global node indices to local ones.\n",
        "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "def train():\n",
        "    memory.train()\n",
        "    gnn.train()\n",
        "    link_pred.train()\n",
        "\n",
        "    memory.reset_state()  # Start with a fresh memory.\n",
        "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
        "\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
        "\n",
        "        # Sample negative destination nodes.\n",
        "        neg_dst = torch.randint(min_dst_idx, max_dst_idx + 1, (src.size(0), ),\n",
        "                                dtype=torch.long, device=device)\n",
        "\n",
        "        n_id = torch.cat([src, pos_dst, neg_dst]).unique()\n",
        "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
        "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
        "\n",
        "        # Get updated memory of all nodes involved in the computation.\n",
        "        z, last_update = memory(n_id)\n",
        "        z = gnn(z, last_update, edge_index, data.t[e_id].to(device),\n",
        "                data.msg[e_id].to(device))\n",
        "\n",
        "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
        "        neg_out = link_pred(z[assoc[src]], z[assoc[neg_dst]])\n",
        "\n",
        "        loss = criterion(pos_out, torch.ones_like(pos_out))\n",
        "        loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
        "\n",
        "        # Update memory and neighbor loader with ground-truth state.\n",
        "        memory.update_state(src, pos_dst, t, msg)\n",
        "        neighbor_loader.insert(src, pos_dst)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        memory.detach()\n",
        "        total_loss += float(loss) * batch.num_events\n",
        "\n",
        "    return total_loss / train_data.num_events\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    memory.eval()\n",
        "    gnn.eval()\n",
        "    link_pred.eval()\n",
        "\n",
        "    torch.manual_seed(12345)  # Ensure deterministic sampling across epochs.\n",
        "\n",
        "    aps, aucs = [], []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
        "\n",
        "        neg_dst = torch.randint(min_dst_idx, max_dst_idx + 1, (src.size(0), ),\n",
        "                                dtype=torch.long, device=device)\n",
        "\n",
        "        n_id = torch.cat([src, pos_dst, neg_dst]).unique()\n",
        "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
        "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
        "\n",
        "        z, last_update = memory(n_id)\n",
        "        z = gnn(z, last_update, edge_index, data.t[e_id].to(device),\n",
        "                data.msg[e_id].to(device))\n",
        "\n",
        "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
        "        neg_out = link_pred(z[assoc[src]], z[assoc[neg_dst]])\n",
        "\n",
        "        y_pred = torch.cat([pos_out, neg_out], dim=0).sigmoid().cpu()\n",
        "        y_true = torch.cat(\n",
        "            [torch.ones(pos_out.size(0)),\n",
        "             torch.zeros(neg_out.size(0))], dim=0)\n",
        "\n",
        "        aps.append(average_precision_score(y_true, y_pred))\n",
        "        aucs.append(roc_auc_score(y_true, y_pred))\n",
        "\n",
        "        memory.update_state(src, pos_dst, t, msg)\n",
        "        neighbor_loader.insert(src, pos_dst)\n",
        "\n",
        "    return float(torch.tensor(aps).mean()), float(torch.tensor(aucs).mean())\n",
        "\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
        "    val_ap, val_auc = test(val_loader)\n",
        "    test_ap, test_auc = test(test_loader)\n",
        "    print(f'Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}')\n",
        "    print(f'Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hlAyGiP1V9l",
        "outputId": "c71dcba6-c4cc-44b1-ca68-225d857e65c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading http://snap.stanford.edu/jodie/wikipedia.csv\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 1.1063\n",
            "Val AP: 0.8582, Val AUC: 0.8776\n",
            "Test AP: 0.8430, Test AUC: 0.8685\n",
            "Epoch: 02, Loss: 0.8341\n",
            "Val AP: 0.9311, Val AUC: 0.9272\n",
            "Test AP: 0.9242, Test AUC: 0.9214\n",
            "Epoch: 03, Loss: 0.6886\n",
            "Val AP: 0.9468, Val AUC: 0.9404\n",
            "Test AP: 0.9400, Test AUC: 0.9353\n",
            "Epoch: 04, Loss: 0.6281\n",
            "Val AP: 0.9530, Val AUC: 0.9484\n",
            "Test AP: 0.9466, Test AUC: 0.9423\n",
            "Epoch: 05, Loss: 0.6087\n",
            "Val AP: 0.9556, Val AUC: 0.9517\n",
            "Test AP: 0.9496, Test AUC: 0.9460\n",
            "Epoch: 06, Loss: 0.5887\n",
            "Val AP: 0.9570, Val AUC: 0.9526\n",
            "Test AP: 0.9524, Test AUC: 0.9481\n",
            "Epoch: 07, Loss: 0.5730\n",
            "Val AP: 0.9579, Val AUC: 0.9534\n",
            "Test AP: 0.9505, Test AUC: 0.9470\n",
            "Epoch: 08, Loss: 0.5621\n",
            "Val AP: 0.9587, Val AUC: 0.9543\n",
            "Test AP: 0.9510, Test AUC: 0.9485\n",
            "Epoch: 09, Loss: 0.5484\n",
            "Val AP: 0.9587, Val AUC: 0.9552\n",
            "Test AP: 0.9531, Test AUC: 0.9513\n",
            "Epoch: 10, Loss: 0.5403\n",
            "Val AP: 0.9608, Val AUC: 0.9572\n",
            "Test AP: 0.9551, Test AUC: 0.9534\n",
            "Epoch: 11, Loss: 0.5318\n",
            "Val AP: 0.9620, Val AUC: 0.9578\n",
            "Test AP: 0.9523, Test AUC: 0.9512\n",
            "Epoch: 12, Loss: 0.5221\n",
            "Val AP: 0.9635, Val AUC: 0.9601\n",
            "Test AP: 0.9563, Test AUC: 0.9542\n",
            "Epoch: 13, Loss: 0.5146\n",
            "Val AP: 0.9633, Val AUC: 0.9599\n",
            "Test AP: 0.9548, Test AUC: 0.9533\n",
            "Epoch: 14, Loss: 0.5073\n",
            "Val AP: 0.9642, Val AUC: 0.9604\n",
            "Test AP: 0.9562, Test AUC: 0.9534\n",
            "Epoch: 15, Loss: 0.5048\n",
            "Val AP: 0.9644, Val AUC: 0.9610\n",
            "Test AP: 0.9576, Test AUC: 0.9550\n",
            "Epoch: 16, Loss: 0.4987\n",
            "Val AP: 0.9645, Val AUC: 0.9608\n",
            "Test AP: 0.9559, Test AUC: 0.9538\n",
            "Epoch: 17, Loss: 0.4897\n",
            "Val AP: 0.9649, Val AUC: 0.9612\n",
            "Test AP: 0.9586, Test AUC: 0.9558\n",
            "Epoch: 18, Loss: 0.4865\n",
            "Val AP: 0.9655, Val AUC: 0.9622\n",
            "Test AP: 0.9591, Test AUC: 0.9560\n",
            "Epoch: 19, Loss: 0.4767\n",
            "Val AP: 0.9661, Val AUC: 0.9625\n",
            "Test AP: 0.9605, Test AUC: 0.9581\n",
            "Epoch: 20, Loss: 0.4725\n",
            "Val AP: 0.9665, Val AUC: 0.9635\n",
            "Test AP: 0.9592, Test AUC: 0.9573\n",
            "Epoch: 21, Loss: 0.4676\n",
            "Val AP: 0.9638, Val AUC: 0.9614\n",
            "Test AP: 0.9573, Test AUC: 0.9559\n",
            "Epoch: 22, Loss: 0.4647\n",
            "Val AP: 0.9690, Val AUC: 0.9650\n",
            "Test AP: 0.9625, Test AUC: 0.9597\n",
            "Epoch: 23, Loss: 0.4591\n",
            "Val AP: 0.9682, Val AUC: 0.9645\n",
            "Test AP: 0.9607, Test AUC: 0.9581\n",
            "Epoch: 24, Loss: 0.4510\n",
            "Val AP: 0.9685, Val AUC: 0.9645\n",
            "Test AP: 0.9619, Test AUC: 0.9583\n",
            "Epoch: 25, Loss: 0.4457\n",
            "Val AP: 0.9690, Val AUC: 0.9652\n",
            "Test AP: 0.9634, Test AUC: 0.9599\n",
            "Epoch: 26, Loss: 0.4414\n",
            "Val AP: 0.9683, Val AUC: 0.9646\n",
            "Test AP: 0.9615, Test AUC: 0.9578\n",
            "Epoch: 27, Loss: 0.4390\n",
            "Val AP: 0.9690, Val AUC: 0.9654\n",
            "Test AP: 0.9645, Test AUC: 0.9611\n",
            "Epoch: 28, Loss: 0.4315\n",
            "Val AP: 0.9695, Val AUC: 0.9657\n",
            "Test AP: 0.9630, Test AUC: 0.9595\n",
            "Epoch: 29, Loss: 0.4306\n",
            "Val AP: 0.9690, Val AUC: 0.9659\n",
            "Test AP: 0.9635, Test AUC: 0.9605\n",
            "Epoch: 30, Loss: 0.4222\n",
            "Val AP: 0.9702, Val AUC: 0.9668\n",
            "Test AP: 0.9651, Test AUC: 0.9619\n",
            "Epoch: 31, Loss: 0.4176\n",
            "Val AP: 0.9686, Val AUC: 0.9658\n",
            "Test AP: 0.9625, Test AUC: 0.9599\n",
            "Epoch: 32, Loss: 0.4165\n",
            "Val AP: 0.9702, Val AUC: 0.9668\n",
            "Test AP: 0.9632, Test AUC: 0.9605\n",
            "Epoch: 33, Loss: 0.4120\n",
            "Val AP: 0.9692, Val AUC: 0.9665\n",
            "Test AP: 0.9627, Test AUC: 0.9606\n",
            "Epoch: 34, Loss: 0.4136\n",
            "Val AP: 0.9698, Val AUC: 0.9674\n",
            "Test AP: 0.9633, Test AUC: 0.9611\n",
            "Epoch: 35, Loss: 0.4074\n",
            "Val AP: 0.9693, Val AUC: 0.9662\n",
            "Test AP: 0.9622, Test AUC: 0.9594\n",
            "Epoch: 36, Loss: 0.4015\n",
            "Val AP: 0.9699, Val AUC: 0.9665\n",
            "Test AP: 0.9635, Test AUC: 0.9614\n",
            "Epoch: 37, Loss: 0.3996\n",
            "Val AP: 0.9704, Val AUC: 0.9679\n",
            "Test AP: 0.9630, Test AUC: 0.9605\n",
            "Epoch: 38, Loss: 0.3961\n",
            "Val AP: 0.9707, Val AUC: 0.9674\n",
            "Test AP: 0.9644, Test AUC: 0.9612\n",
            "Epoch: 39, Loss: 0.3921\n",
            "Val AP: 0.9695, Val AUC: 0.9670\n",
            "Test AP: 0.9630, Test AUC: 0.9607\n",
            "Epoch: 40, Loss: 0.3914\n",
            "Val AP: 0.9708, Val AUC: 0.9673\n",
            "Test AP: 0.9646, Test AUC: 0.9607\n",
            "Epoch: 41, Loss: 0.3903\n",
            "Val AP: 0.9704, Val AUC: 0.9671\n",
            "Test AP: 0.9629, Test AUC: 0.9600\n",
            "Epoch: 42, Loss: 0.3900\n",
            "Val AP: 0.9705, Val AUC: 0.9676\n",
            "Test AP: 0.9641, Test AUC: 0.9613\n",
            "Epoch: 43, Loss: 0.3863\n",
            "Val AP: 0.9695, Val AUC: 0.9669\n",
            "Test AP: 0.9590, Test AUC: 0.9583\n",
            "Epoch: 44, Loss: 0.3806\n",
            "Val AP: 0.9694, Val AUC: 0.9669\n",
            "Test AP: 0.9620, Test AUC: 0.9596\n",
            "Epoch: 45, Loss: 0.3831\n",
            "Val AP: 0.9702, Val AUC: 0.9675\n",
            "Test AP: 0.9635, Test AUC: 0.9602\n",
            "Epoch: 46, Loss: 0.3836\n",
            "Val AP: 0.9682, Val AUC: 0.9655\n",
            "Test AP: 0.9624, Test AUC: 0.9597\n",
            "Epoch: 47, Loss: 0.3734\n",
            "Val AP: 0.9693, Val AUC: 0.9668\n",
            "Test AP: 0.9608, Test AUC: 0.9594\n",
            "Epoch: 48, Loss: 0.3721\n",
            "Val AP: 0.9691, Val AUC: 0.9667\n",
            "Test AP: 0.9627, Test AUC: 0.9602\n",
            "Epoch: 49, Loss: 0.3696\n",
            "Val AP: 0.9685, Val AUC: 0.9667\n",
            "Test AP: 0.9605, Test AUC: 0.9585\n",
            "Epoch: 50, Loss: 0.3697\n",
            "Val AP: 0.9697, Val AUC: 0.9669\n",
            "Test AP: 0.9607, Test AUC: 0.9582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JRGP4g9Xb8uD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tgn_dgl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}